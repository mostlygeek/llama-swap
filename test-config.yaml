# yaml-language-server: $schema=https://raw.githubusercontent.com/mostlygeek/llama-swap/refs/heads/main/config-schema.json
#
# llama-swap configuration for 16GB VRAM AMD Radeon RX 6800 XT (gfx1030)
# Optimized for headless system with no display overhead
# -------------------------------------

healthCheckTimeout: 300
logLevel: info
logTimeFormat: "rfc3339"
logToStdout: "proxy"
metricsMaxInMemory: 1000
startPort: 10001
sendLoadingState: false
includeAliasesInList: false

macros:
  "latest-llama": >
    /home/svc-gpgpu/.local/bin/llama-server
    --port ${PORT} --host 0.0.0.0 -b 512 -ub 32 -np 1
  "default_ctx": 4096
  "rocm_device": "0"

models:
  # ========================================
  # GENERAL PURPOSE MODELS
  # ========================================

  "qwen3:14b-q5_k_m-32768":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen3-14B-GGUF:q5_k_m
      --ctx-size 32768
      -fa auto
      -ctv q8_0
      -ctk q8_0
      -ngl 99
      --jinja
      --mmap
      -b 512
    name: "qwen3:14b-q5_k_m-32768"
    description: "VRAM: 12505 MiB"
    ttl: 600

  "qwen3:8b-q5_k_m-40960":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen3-8B-GGUF:q5_k_m
      --ctx-size 40960
      -fa auto
      -ctv q8_0
      -ctk q8_0
      -ngl 99
      --jinja
      --mmap
      -b 512
    name: "qwen3:8b-q5_k_m-40960"
    description: "VRAM: 8491 MiB"
    ttl: 600

  "qwen3:8b-q8_0-32768":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen3-8B-GGUF:q8_0
      --ctx-size 32768
      -fa auto
      -ctv q8_0
      -ctk q8_0
      -ngl 99
      --jinja
      --mmap
      -b 512
    name: "qwen3:8b-q8_0-32768"
    description: "VRAM: 10381 MiB"
    ttl: 600

  "ministral-3:14b-instruct-q5_k_m-20480-vision":
    cmd: |
      ${latest-llama}
      -hf mistralai/Ministral-3-14B-Instruct-2512-GGUF:q5_k_m
      --ctx-size 20480
      -fa off
      -ngl 99
      --mmap
      --jinja
      --mmproj-auto
    name: "ministral-3:14b-instruct-q5_k_m-20480-vision"
    description: "VRAM: 13184 MiB"
    ttl: 600

  "ministral-3:14b-reasoning-q5_k_m-20480-vision":
    cmd: |
      ${latest-llama}
      -hf mistralai/Ministral-3-14B-Reasoning-2512-GGUF:q5_k_m
      --ctx-size 20480
      -fa off
      -ngl 99
      --mmap
      --jinja
      --mmproj-auto
    name: "ministral-3:14b-reasoning-q5_k_m-20480-vision"
    description: "VRAM: 13184 MiB"
    ttl: 600

  "ministral-3:14b-instruct-q5_k_m-32768":
    cmd: |
      ${latest-llama}
      -hf mistralai/Ministral-3-14B-Instruct-2512-GGUF:q5_k_m
      --ctx-size 32768
      -fa off
      -ngl 99
      --mmap
      --jinja
      --no-mmproj
    name: "ministral-3:14b-instruct-q5_k_m-32768"
    description: "VRAM: 14224 MiB"
    ttl: 600

  "ministral-3:14b-reasoning-q5_k_m-32768":
    cmd: |
      ${latest-llama}
      -hf mistralai/Ministral-3-14B-Reasoning-2512-GGUF:q5_k_m
      --ctx-size 32768
      -fa off
      -ngl 99
      --mmap
      --jinja
      --no-mmproj
    name: "ministral-3:14b-reasoning-q5_k_m-32768"
    description: "VRAM: 14224 MiB"
    ttl: 600

  # ========================================
  # UTILITY MODELS (General Purpose)
  # ========================================

  "embeddinggemma:300m":
    cmd: |
      ${latest-llama}
      -hf gaianet/embeddinggemma-300m-GGUF
      --ctx-size 2048
      -fa off
      -ngl 99
      --embeddings
      --pooling mean
      -b 1024
      -ub 1024
    name: "embeddinggemma:300m"
    description: "VRAM: 512 MiB"
    ttl: 3600

  "bge-reranker-v2-m3":
    cmd: |
      ${latest-llama}
      -hf Felladrin/bge-reranker-v2-m3-Q8_0-GGUF
      --ctx-size 8192
      -ngl 99
      --mmap
      --rerank
      --embedding
      --pooling rank
      -b 8192
      -ub 8192
    name: "bge-reranker-v2-m3"
    description: "VRAM: 1077 MiB"
    ttl: 3600

  # ========================================
  # CODING MODELS
  # ========================================

  "qwen2.5-coder:14b-q5_k_m-32768":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:q5_k_m
      --ctx-size 32768
      -fa auto
      -ctv q8_0
      -ctk q8_0
      -ngl 99
      --jinja
      --mmap
      -b 512
    name: "qwen2.5-coder:14b-q5_k_m-32768"
    description: "VRAM: ~12500 MiB"
    ttl: 600

  "qwen2.5-coder:1.5b-q4_k_m-autocomplete":
    cmd: |
      ${latest-llama}
      -hf Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF:q4_k_m
      --ctx-size 2048
      -fa off
      -ngl 99
      -b 128
      -ub 32
      --mmap
      --no-warmup
    name: "qwen2.5-coder:1.5b-q4_k_m-autocomplete"
    description: "VRAM: ~1000 MiB"
    ttl: 3600

  # ========================================
  # PERSISTENT CPU MODEL
  # ========================================

  "qwen3:1.7b-cpu-json":
    cmd: |
      ${latest-llama}
      -hf unsloth/Qwen3-1.7B-GGUF:Q4_K_M
      --ctx-size 8192
      -fa off
      -ngl 0
      -b 512
      --jinja
      --mmap
    name: "qwen3:1.7b-cpu-json"
    description: "CPU-only - permanent RAM resident for tags/titles/queries"
    ttl: 0

# ========================================
# GROUPS CONFIGURATION
# ========================================

groups:
  # General purpose models can coexist with utility models
  # When loaded, they prevent coding group from running
  "general-purpose":
    swap: false       # All models in group can run simultaneously
    exclusive: true   # Unloads other exclusive groups when active
    members:
      - "qwen3:14b-q5_k_m-32768"
      - "qwen3:8b-q5_k_m-40960"
      - "qwen3:8b-q8_0-32768"
      - "ministral-3:14b-instruct-q5_k_m-20480-vision"
      - "ministral-3:14b-reasoning-q5_k_m-20480-vision"
      - "ministral-3:14b-instruct-q5_k_m-32768"
      - "ministral-3:14b-reasoning-q5_k_m-32768"
      - "bge-reranker-v2-m3"

  # Coding models can coexist with each other
  # When loaded, they prevent general-purpose group from running
  "coding":
    swap: false       # Both coder models can run simultaneously
    exclusive: true   # Unloads other exclusive groups when active
    members:
      - "qwen2.5-coder:14b-q5_k_m-32768"
      - "qwen2.5-coder:1.5b-q4_k_m-autocomplete"

  # CPU-based persistent model - never unloaded, doesn't interfere
  "persistent-cpu":
    swap: false       # No swapping (only one model anyway)
    exclusive: false  # Doesn't unload other groups
    persistent: true  # Other groups cannot unload this
    members:
      - "qwen3:1.7b-cpu-json"

# ========================================
# STARTUP HOOKS
# ========================================

hooks:
  on_startup:
    preload:
      - "qwen3:1.7b-cpu-json"
