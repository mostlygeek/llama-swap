groups:
    big-ones:
        exclusive: true
        members:
            - MiniMax-M2.5-AWQ
            - MiniMax-M2.5-AWQ-Optimized
            - Qwen/Qwen3-Coder-Next-FP8
            - gpt-oss-120b
        swap: true
    medium:
        exclusive: true
        members:
            - GLM-4.7-Flash-AWQ-4bit
            - Nemotron-3-Nano-NVFP4
        swap: true
healthCheckTimeout: 600
hooks: null
macros:
    llama_root: /home/csolutions_ai/Swap-Laboratories
    recipe_runner: /home/csolutions_ai/spark-vllm-docker/run-recipe.sh
    spark_root: /home/csolutions_ai/spark-vllm-docker
    user_home: ${env.HOME}
    vllm_container: vllm_node
    vllm_exec_env: -e VLLM_HOST_IP=${vllm_head_ip} -e RAY_NODE_IP_ADDRESS=${vllm_head_ip} -e RAY_OVERRIDE_NODE_IP_ADDRESS=${vllm_head_ip} -e NCCL_SOCKET_IFNAME=${vllm_iface} -e GLOO_SOCKET_IFNAME=${vllm_iface} -e TP_SOCKET_IFNAME=${vllm_iface}
    vllm_head_ip: 192.168.200.12
    vllm_iface: enp1s0f1np1
    vllm_marlin_image: vllm-node-marlin-sm12x
    vllm_nodes: 192.168.200.12,192.168.200.13
    vllm_stop_cluster: |-
        for n in $(echo "${vllm_nodes}" | tr "," " "); do
          n="$(echo "$n" | xargs)";
          if [ -z "$n" ]; then continue; fi;
          if [ "$n" = "${vllm_head_ip}" ]; then continue; fi;
          ssh -o BatchMode=yes -o ConnectTimeout=5 -o StrictHostKeyChecking=no "$n" "docker rm -f ${vllm_container} >/dev/null 2>&1 || true" || true;
        done; docker rm -f ${vllm_container} >/dev/null 2>&1 || true
models:
    GLM-4.7-Flash-AWQ-4bit:
        aliases:
            - cyankiwi/GLM-4.7-Flash-AWQ-4bit
            - glm-4.7-flash
        checkEndpoint: /health
        cmd: bash -lc '${vllm_stop_cluster}; exec ${recipe_runner} glm-4.7-flash-awq -n ${vllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: |
            bash -lc '${vllm_stop_cluster}'
        description: vLLM cyankiwi--AWQ
        name: GLM-4.7-Flash-AWQ-4bit
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: glm-4.7-flash
    MiniMax-M2.5-AWQ:
        aliases:
            - QuantTrio/MiniMax-M2.5-AWQ
        checkEndpoint: /health
        cmd: bash -lc '${vllm_stop_cluster}; exec ${recipe_runner} minimax-m2.5-awq -n ${vllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: |
            bash -lc '${vllm_stop_cluster}'
        description: vLLM QuantTrio--AWQ
        name: MiniMax-M2.5-AWQ
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: minimax-m2.5-awq
    MiniMax-M2.5-AWQ-Optimized:
        aliases:
            - QuantTrio/MiniMax-M2.5-AWQ-optimized
            - minimax-m2.5-awq-optimized
        checkEndpoint: /health
        cmd: bash -lc '${vllm_stop_cluster}; export VLLM_SPARK_EXTRA_DOCKER_ARGS="-e FLASHINFER_DISABLE_VERSION_CHECK=1 -e VLLM_MARLIN_INPUT_DTYPE=fp8 -e VLLM_MARLIN_USE_ATOMIC_ADD=1 -e VLLM_USE_FLASHINFER_MOE_FP16=1 -e VLLM_USE_FLASHINFER_SAMPLER=1"; exec ${recipe_runner} ${llama_root}/recipes/minimax-m2.5-awq-optimized-marlin.yaml -t ${vllm_marlin_image} -n ${vllm_nodes} --tp 2 --gpu-mem 0.88 --max-model-len 185000 --port ${PORT}'
        cmdStop: |
            bash -lc '${vllm_stop_cluster}'
        description: vLLM QuantTrio--AWQ (optimized)
        name: MiniMax-M2.5-AWQ-Optimized
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: minimax-m2.5-awq
    Nemotron-3-Nano-NVFP4:
        aliases:
            - nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4-nano
        checkEndpoint: /health
        cmd: bash -lc '${vllm_stop_cluster}; exec ${recipe_runner} nemotron-3-nano-nvfp4 --solo --port ${PORT}'
        cmdStop: |
            bash -lc '${vllm_stop_cluster}'
        description: vLLM Nemotron nano recipe with custom parser/mods
        metadata:
            benchy:
                trust_remote_code: true
        name: Nemotron-3-Nano-NVFP4 (solo recipe)
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4
    Qwen/Qwen3-Coder-Next-FP8:
        aliases:
            - Qwen/Qwen3-Coder-Next-FP8
        checkEndpoint: /health
        cmd: bash -lc '${vllm_stop_cluster}; exec ${recipe_runner} qwen3-coder-next-fp8 -n ${vllm_nodes} --tp 2 --port ${PORT} --gpu-mem 0.7 --max-model-len 185000 -- --enable-prefix-caching'
        cmdStop: bash -lc '${vllm_stop_cluster}'
        description: vLLM Qwen3 NEXT FP8
        metadata:
            benchy:
                trust_remote_code: true
            recipe_ui:
                backend_dir: /home/csolutions_ai/spark-vllm-docker
                extra_args: --gpu-mem 0.7 --max-model-len 185000 -- --enable-prefix-caching
                group: big-ones
                managed: true
                mode: cluster
                nodes: ${vllm_nodes}
                recipe_ref: qwen3-coder-next-fp8
                tensor_parallel: 2
        name: Qwen/Qwen3-Coder-Next-FP8
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        unlisted: false
        useModelName: Qwen/Qwen3-Coder-Next-FP8
    gpt-oss-120b:
        aliases:
            - openai/gpt-oss-120b
        checkEndpoint: /health
        cmd: bash -lc '${vllm_stop_cluster}; exec ${recipe_runner} openai-gpt-oss-120b -n ${vllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: |
            bash -lc '${vllm_stop_cluster}'
        description: vLLM MXFP4
        name: GPT-OSS 120B
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: openai/gpt-oss-120b
startPort: 8089
